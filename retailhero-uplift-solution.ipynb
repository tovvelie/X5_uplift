{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "from datetime import date, datetime\n",
    "from random import normalvariate\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from catboost import CatBoostClassifier\n",
    "import lightgbm as lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products = pd.read_csv('data/products.csv', index_col='product_id')\n",
    "df_transactions = pd.read_csv('data/purchases.csv').join(df_products['is_own_trademark'].astype(np.int8), on='product_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_transactions.columns:\n",
    "    if df_transactions[col].dtype == 'float64':\n",
    "        df_transactions[col] = df_transactions[col].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "own_trademark_sum = df_transactions[df_transactions['is_own_trademark'] == 1].groupby('client_id')['trn_sum_from_iss'].sum().rename('own_trademark_sum')\n",
    "df_transactions.drop_duplicates(inplace=True)\n",
    "features =  pd.concat([df_transactions.groupby('client_id')['purchase_sum'].count(),\n",
    "                       df_transactions.groupby('client_id')['purchase_sum'].mean(),\n",
    "                       df_transactions.groupby('client_id')['purchase_sum'].std(),\n",
    "                       df_transactions.groupby('client_id')['express_points_spent'].mean(),\n",
    "                       df_transactions.groupby('client_id')['express_points_spent'].std(),\n",
    "                       df_transactions.groupby('client_id')['express_points_received'].mean(),\n",
    "                       df_transactions.groupby('client_id')['express_points_received'].std(),\n",
    "                       df_transactions.groupby('client_id')['regular_points_spent'].mean(),\n",
    "                       df_transactions.groupby('client_id')['regular_points_spent'].std(),\n",
    "                       df_transactions.groupby('client_id')['regular_points_received'].mean(),\n",
    "                       df_transactions.groupby('client_id')['regular_points_received'].std(),\n",
    "                       df_transactions.groupby('client_id')[['store_id']].nunique(),\n",
    "                       df_transactions.groupby('client_id')[['regular_points_received', 'express_points_received', \n",
    "                                                             'regular_points_spent', 'express_points_spent', \n",
    "                                                             'purchase_sum']].sum()\n",
    "                      ],axis = 1)\n",
    "\n",
    "features.columns = ['total_trans_count', 'mean_purchase', 'std_purchase', 'mean_epoints_spent', 'std_epoints_spent',\n",
    "                    'mean_epoints_recd', 'std_epoints_recd', 'mean_rpoints_spent', 'std_rpoints_spent',\n",
    "                    'mean_rpoints_recd', 'std_rpoints_recd', 'nunique_stores'] + \\\n",
    "                    list(c+\"_sum\" for c in ['regular_points_received', 'express_points_received',\n",
    "                                            'regular_points_spent', 'express_points_spent', 'purchase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_month_transactions = df_transactions[df_transactions['transaction_datetime'] > '2019-02-18']\n",
    "last_month_features =  pd.concat([last_month_transactions.groupby('client_id')['purchase_sum'].count(),\n",
    "                                  last_month_transactions.groupby('client_id')[['store_id']].nunique(),\n",
    "                                  last_month_transactions.groupby('client_id')[['regular_points_received', 'express_points_received', \n",
    "                                                                               'regular_points_spent', 'express_points_spent', \n",
    "                                                                               'purchase_sum']].sum()\n",
    "                                 ],axis = 1)\n",
    "\n",
    "last_month_features.columns = list(['total_trans_count', 'nunique_stores'] + \\\n",
    "                                list(c+\"_sum\" for c in ['regular_points_received', 'express_points_received','regular_points_spent', \n",
    "                                                        'express_points_spent', 'purchase']))\n",
    "last_month_features.columns = list(last_month_features.columns + '_last_month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['regular_points_received_ratio'] = features['regular_points_received_sum'] / features['purchase_sum']\n",
    "features['express_points_received_ratio'] = features['express_points_received_sum'] / features['purchase_sum']\n",
    "features['regular_points_spent_ratio'] = features['regular_points_spent_sum'] / features['purchase_sum']\n",
    "features['express_points_spent_ratio'] = features['express_points_spent_sum'] / features['purchase_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.merge(features, own_trademark_sum, how='left', on='client_id')\n",
    "features['own_trademark_ratio'] = features['own_trademark_sum'] / features['purchase_sum']\n",
    "features.drop(columns='own_trademark_sum', inplace=True)\n",
    "df_transactions_features = pd.merge(features, last_month_features, how='inner', on='client_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del [df_products, df_transactions, own_trademark_sum, features, last_month_transactions, last_month_features]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clients = pd.read_csv('data/clients.csv', index_col='client_id',\n",
    "                         parse_dates=['first_issue_date','first_redeem_date'])\n",
    "df_train = pd.read_csv('data/uplift_train.csv', index_col='client_id')\n",
    "df_test = pd.read_csv('data/uplift_test.csv', index_col='client_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to fix age variable in the data. Also, set age_antinorm variable depending on \n",
    "# type of error in the age data.\n",
    "# Set real_fix flag to False to calculate age_antinorm, but not fix the age\n",
    "#\n",
    "# Use the following heuristics:\n",
    "# 19XX means year of birth -> easy to convert to age\n",
    "# 18XX - the same, but it should be 9 instead of 8\n",
    "#  9XX - the same, first '1' is missed\n",
    "# -9XX - the same as 19XX, '1' was OCRed as '-'\n",
    "# etc\n",
    "#\n",
    "def fix_age(df_clients, real_fix=True):\n",
    "\n",
    "    # create a copy of age column. Modify the copy for now\n",
    "    df_clients['age2'] = df_clients['age']\n",
    "    \n",
    "    age_index = (df_clients['age'] < -900) & (df_clients['age'] > -1000)\n",
    "    df_clients.loc[age_index, 'age2'] = -1 * df_clients.loc[age_index, 'age'] + 1019\n",
    "    df_clients.loc[age_index, 'age_antinorm'] = 1\n",
    "\n",
    "    age_index = (df_clients['age'] > 900) & (df_clients['age'] < 1000)\n",
    "    df_clients.loc[age_index, 'age2'] = 1019 - df_clients.loc[age_index, 'age']\n",
    "    df_clients.loc[age_index, 'age_antinorm'] = 2\n",
    "\n",
    "    age_index = (df_clients['age'] > 1900) & (df_clients['age'] < 2000)\n",
    "    df_clients.loc[age_index, 'age2'] = 2019 - df_clients.loc[age_index, 'age']\n",
    "    df_clients.loc[age_index, 'age_antinorm'] = 3\n",
    "    \n",
    "    age_index = (df_clients['age'] > 120) & (df_clients['age'] < 200)\n",
    "    df_clients.loc[age_index, 'age2'] = df_clients.loc[age_index, 'age'] - 100\n",
    "    df_clients.loc[age_index, 'age_antinorm'] = 4\n",
    "\n",
    "    age_index = (df_clients['age'] > 1800) & (df_clients['age'] < 1900)\n",
    "    df_clients.loc[age_index, 'age2'] = df_clients.loc[age_index, 'age'] - 1800\n",
    "    df_clients.loc[age_index, 'age_antinorm'] = 5\n",
    "\n",
    "    # the following types of errors are impossible to recover\n",
    "    # so we set the age to mean of all clients (46), slightly randomizing it (std=16)\n",
    "    age_index = (df_clients['age'] > 120)\n",
    "    df_clients.loc[age_index, 'age2'] = normalvariate(46, 16)\n",
    "    df_clients.loc[age_index, 'age_antinorm'] = 6\n",
    "    \n",
    "    age_index = (df_clients['age'] > 0) & (df_clients['age'] < 12)\n",
    "    df_clients.loc[age_index, 'age2'] = normalvariate(46, 16)\n",
    "    df_clients.loc[age_index, 'age_antinorm'] = 7\n",
    "    \n",
    "    age_index = (df_clients['age'] == 0)\n",
    "    df_clients.loc[age_index, 'age2'] = normalvariate(46, 16)\n",
    "    df_clients.loc[age_index, 'age_antinorm'] = 8\n",
    "    \n",
    "    age_index = (df_clients['age'] < 0)\n",
    "    df_clients.loc[age_index, 'age2'] = normalvariate(46, 16)\n",
    "    df_clients.loc[age_index, 'age_antinorm'] = 9\n",
    "    \n",
    "    # use the modified copy \n",
    "    if (real_fix):\n",
    "        df_clients['age'] = df_clients['age2']\n",
    "    \n",
    "    df_clients.drop('age2', axis=1, inplace=True)\n",
    "    \n",
    "    return df_clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clients['age_antinorm'] = 0\n",
    "df_clients = fix_age(df_clients)\n",
    "\n",
    "df_clients = pd.get_dummies(df_clients, prefix='gender', columns=['gender'])\n",
    "df_clients['first_issue_unixtime'] = (df_clients['first_issue_date']).astype(int)/10**9\n",
    "df_clients['first_redeem_unixtime'] = (df_clients['first_redeem_date']).astype(int)/10**9\n",
    "\n",
    "df_clients['issue_redeem_delay'] = df_clients['first_redeem_unixtime'] - df_clients['first_issue_unixtime']\n",
    "df_clients.drop(columns=['first_issue_date', 'first_redeem_date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataset(X, to_merge):\n",
    "    \n",
    "    result = X\n",
    "    \n",
    "    for table in to_merge:\n",
    "        \n",
    "        result = pd.merge(result, table, how='inner', on='client_id')\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = merge_dataset(df_train, [df_clients, df_transactions_features])\n",
    "train_X['new_target'] = (train_X['target'] + train_X['treatment_flg'] + 1) % 2\n",
    "test_X = merge_dataset(df_test, [df_clients, df_transactions_features])\n",
    "treatment = train_X[train_X['treatment_flg'] == 1].drop('treatment_flg', axis=1)\n",
    "treatment_X = treatment.drop(['target', 'new_target'], axis=1, errors='ignore')\n",
    "treatment_y = treatment['target']\n",
    "control = train_X[train_X['treatment_flg'] == 0].drop('treatment_flg', axis=1)\n",
    "control_X = control.drop(['target', 'new_target'], axis=1, errors='ignore')\n",
    "control_y = control['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_train_model(model, df_X, df_X_test, num_folds=5, random_state=0, verbose=2):\n",
    "\n",
    "    cat_params['random_state'] = random_state\n",
    "    lgbm_params['random_state'] = random_state\n",
    "\n",
    "    df_X['new_target'] = (df_X['target'] + df_X['treatment_flg'] + 1) % 2\n",
    "    df_y = df_X[['new_target']]\n",
    "    treatment = df_X['treatment_flg'].to_numpy()\n",
    "    old_target = df_X['target'].to_numpy()\n",
    "    df_X = df_X.drop(['target', 'new_target', 'treatment_flg'], axis=1, errors='ignore')    \n",
    "    \n",
    "    X = df_X.to_numpy()\n",
    "    y = df_y.to_numpy()\n",
    "    X_test = df_X_test.to_numpy()\n",
    "    \n",
    "    folds = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=random_state)\n",
    "    scores = []\n",
    "    uplift_scores = []    \n",
    "    prediction = np.zeros(len(X_test))\n",
    "    feature_importances = []\n",
    "\n",
    "    for i, (train_index, valid_index) in enumerate(folds.split(X, y)):    \n",
    "        X_train, X_valid = X[train_index], X[valid_index]\n",
    "        y_train, y_valid = y[train_index], y[valid_index]        \n",
    "        treat_valid = treatment[valid_index]\n",
    "        old_target_vaild = old_target[valid_index]\n",
    "        \n",
    "        if (model == 'catboost'):\n",
    "            f = CatBoostClassifier(**cat_params)\n",
    "            f.fit(X_train, y_train, eval_set=(X_valid, y_valid), use_best_model=True, verbose=False)\n",
    "        elif (model == 'lgbm'):\n",
    "            f = lgbm.LGBMClassifier(**lgbm_params)        \n",
    "            f.fit(X_train, y_train, eval_set=(X_valid, y_valid), verbose=False)\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        feature_importances.append(f.feature_importances_)\n",
    "        y_pred_valid = f.predict_proba(X_valid)[:, 1]\n",
    "        score = log_loss(y_valid, y_pred_valid)\n",
    "        uplift_score = uplift_at_k(old_target_vaild, y_pred_valid, treat_valid)\n",
    "        uplift_scores.append(uplift_score)\n",
    "        if (verbose > 1):\n",
    "            print('Uplift score: {0:.5f}'.format(uplift_score))\n",
    "        \n",
    "        scores.append(score)\n",
    "        # predict on test and accumulate the result\n",
    "        y_pred = f.predict_proba(X_test)[:, 1]\n",
    "        prediction += y_pred\n",
    "\n",
    "    # get average prediction from all models\n",
    "    prediction /= num_folds\n",
    "    importances_mean = np.mean(feature_importances, axis=0)\n",
    "    df_features = pd.DataFrame(zip(importances_mean, df_X.columns), columns=['Value','Feature']).sort_values(by='Value', ascending=False)\n",
    "\n",
    "    if (verbose > 0):\n",
    "        print('CV mean score: {0:.5f}, std: {1:.5f}'.format(np.mean(scores), np.std(scores)))    \n",
    "        print('Uplift score @30%: {0:.5f}, std: {1:.5f}'.format(np.mean(uplift_scores), np.std(uplift_scores)))\n",
    "        print(df_features)\n",
    "        \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uplift_at_k(y_true, uplift, treatment, k=0.3):\n",
    "    \"\"\"Compute uplift at first k percentage of the total sample.\n",
    "\n",
    "    Args:\n",
    "        y_true (1d array-like): Ground truth (correct) labels.\n",
    "        uplift (1d array-like): Predicted uplift, as returned by a model.\n",
    "        treatment (1d array-like): Treatment labels.\n",
    "        k (float > 0 and <= 1): Percentage of the total sample to compute uplift.\n",
    "\n",
    "    Returns:\n",
    "        float: Uplift at first k percentage of the total sample.\n",
    "\n",
    "    Reference:\n",
    "        Baseline from `RetailHero competition`_.\n",
    "\n",
    "    .. _RetailHero competition:\n",
    "        https://retailhero.ai/c/uplift_modeling/overview\n",
    "    \"\"\"\n",
    "    order = np.argsort(-uplift)\n",
    "    treatment_n = int((treatment == 1).sum() * k)\n",
    "    treatment_p = y_true[order][treatment[order] == 1][:treatment_n].mean()\n",
    "    control_n = int((treatment == 0).sum() * k)\n",
    "    control_p = y_true[order][treatment[order] == 0][:control_n].mean()\n",
    "    score_at_k = treatment_p - control_p\n",
    "    \n",
    "    return score_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_params = {'learning_rate':0.01, 'max_depth':3,\n",
    "              'loss_function':'Logloss', 'eval_metric':'Logloss',\n",
    "               'iterations':20000, 'od_type': \"Iter\", 'od_wait':200\n",
    "}\n",
    "\n",
    "lgbm_params = {'learning_rate':0.01,'max_depth':6,'num_leaves':20, 'min_data_in_leaf':3, \n",
    "               'subsample':0.8, 'colsample_bytree': 0.8, 'reg_alpha':0.01,'max_bin':416,\n",
    "               'bagging_freq':3,'reg_lambda':0.01,'num_leaves':20, 'n_estimators':600, \n",
    "               'eval_metric':'Logloss', 'application':'binary', \n",
    "               'iterations':20000, 'od_type': 'Iter', 'od_wait':200\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uplift score: 0.07111\n",
      "Uplift score: 0.08943\n",
      "Uplift score: 0.08871\n",
      "Uplift score: 0.07330\n",
      "Uplift score: 0.07869\n",
      "CV mean score: 0.68845, std: 0.00029\n",
      "Uplift score @30%: 0.08025, std: 0.00762\n",
      "        Value                                 Feature\n",
      "6   70.513373                   first_redeem_unixtime\n",
      "0    2.583397                                     age\n",
      "28   1.990594              express_points_spent_ratio\n",
      "7    1.460207                      issue_redeem_delay\n",
      "9    1.266638                           mean_purchase\n",
      "5    1.257648                    first_issue_unixtime\n",
      "24   1.230376                            purchase_sum\n",
      "36   1.152020                 purchase_sum_last_month\n",
      "8    1.131740                       total_trans_count\n",
      "23   1.096668                express_points_spent_sum\n",
      "30   1.086737            total_trans_count_last_month\n",
      "32   1.069755  regular_points_received_sum_last_month\n",
      "12   1.005317                       std_epoints_spent\n",
      "25   0.981327           regular_points_received_ratio\n",
      "29   0.970925                     own_trademark_ratio\n",
      "20   0.902409             regular_points_received_sum\n",
      "35   0.882090     express_points_spent_sum_last_month\n",
      "10   0.874375                            std_purchase\n",
      "18   0.842646                        std_rpoints_recd\n",
      "17   0.817631                       mean_rpoints_recd\n",
      "11   0.771718                      mean_epoints_spent\n",
      "34   0.755843     regular_points_spent_sum_last_month\n",
      "27   0.713800              regular_points_spent_ratio\n",
      "19   0.625250                          nunique_stores\n",
      "22   0.619024                regular_points_spent_sum\n",
      "16   0.596055                       std_rpoints_spent\n",
      "31   0.502925               nunique_stores_last_month\n",
      "15   0.460466                      mean_rpoints_spent\n",
      "2    0.367751                                gender_F\n",
      "21   0.270336             express_points_received_sum\n",
      "26   0.269661           express_points_received_ratio\n",
      "13   0.235310                       mean_epoints_recd\n",
      "14   0.209250                        std_epoints_recd\n",
      "4    0.190613                                gender_U\n",
      "33   0.159109  express_points_received_sum_last_month\n",
      "1    0.104067                            age_antinorm\n",
      "3    0.032949                                gender_M\n"
     ]
    }
   ],
   "source": [
    "pred = trans_train_model('catboost', train_X, test_X, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.DataFrame({'client_id':test_X.index.values,'uplift': pred})\n",
    "df_submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
